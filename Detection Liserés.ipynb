{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275b6713-abd8-4e90-9e23-8f5ec9b7f15c",
   "metadata": {},
   "source": [
    "# Processus complet de detection de liserés\n",
    "\n",
    "Voici tout le processus de détection automatique des liserés contenu en un seul fichier. Il sera amélioré au fur et à mesure, mais les perofrmances dépendent surtout des réseaux de neurones utilisés, et pas vraiment du code contenu dans ce notebook.\n",
    "\n",
    "Ce notebook utilise:\n",
    "\n",
    "- Une grande carte (c.à.d. une image reconstruite représentant une grande surface d'échantillon à analyser)\n",
    "- Un réseau de neurones\n",
    "\n",
    "Ce notebook produit:\n",
    "\n",
    "- Une image qui correspond à la superposition de la grande carte donnée en entrée, et des prédictions faites par le réseau de neurones sur cette carte\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. **Paramètres**\n",
    "\n",
    "- Un ensemble de variables à modifier pour faire configurer le processus. C'est normalement la seule partie du code que vous devrez manipuler.\n",
    "\n",
    "2. **Imports et fonctions utilitaires**\n",
    "\n",
    "- Importe toutes les bibliothèques nécéssaires et déclare des fonctions/classes utilitaires qui servent pour le reste du notebook\n",
    "\n",
    "3. **Découpage de la carte**\n",
    "\n",
    "- Permet de découper une grande carte en un jeu de données adapté au réseau de neurones. Ne doit être executé qu'une seule fois pour chaque carte.\n",
    "\n",
    "4. **Chargement du réseau de neurones**\n",
    "\n",
    "- Charge le réseau de neurones. Le construit automatiquement à partir de ses poids si nécéssaire.\n",
    "   \n",
    "5. **Prédiction sur la carte**\n",
    "\n",
    "- Effectue l'analyse de la grande carte par le réseau de neurones\n",
    "\n",
    "6. **Superposition et sauvegarde**\n",
    "\n",
    "- Superpose la prédiction à la carte, et effectue la sauvegarde. Cette section peut être ré-exécutée après avoir modifié certains paramètres. \n",
    "\n",
    "## Comment utiliser ce notebook ?\n",
    "\n",
    "Allez dans la section **1. Paramètres** et modifiez les paramètres appropriés, puis appuyez sur le bouton *Restart Kernel and Run all Cells* (ou quelque chose qui y ressemble, en fonction de la version de jupyter utilisée) en haut du notebook.\n",
    "\n",
    "## Autres informations\n",
    "\n",
    "- En python, pour commenter une ligne, on utilise le symbole *#* (équivalent au *%* de matlab)\n",
    "- Pour exécuter une seule cellule, selectionnez la cellule et utilisez le raccourci **Majuscule+Entrée**. Cela fonctionne aussi avec les cellules en *markdown* comme celle qui contient ce texte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326c0eb-5b30-4aa3-a292-062a35f0323f",
   "metadata": {},
   "source": [
    "# 1. Paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80d0c2-23ef-4056-a4aa-a61bb475c4d8",
   "metadata": {},
   "source": [
    "## 1.1 Paramètres de la grande carte et du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f2dc2-95a9-4cb1-baaf-b0dd269c2c58",
   "metadata": {},
   "source": [
    "***\n",
    "**Création du dataset**\n",
    "\n",
    "L'opération de découpage de la grande carte en dataset n'a besoin d'être effectuée qu'une seule fois. Elle découpe la grande image en entrée en une collection de plus petites images (le *dataset*), et les enregistre sur votre ordinateur.\n",
    "\n",
    "Pour ne pas effectuer cette opération (car le dataset a déjà été généré):\n",
    "> creation_dataset = False\n",
    "\n",
    "Pour effectuer cette opération (première fois que vous analysez cette grande carte):\n",
    "> creation_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4aa27-520d-4052-9cfb-5cf1c09c63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_dataset = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8530b0a-e980-4f35-be66-411a61f1b35b",
   "metadata": {},
   "source": [
    "***\n",
    "**Chemin vers le dossier qui contient le dataset.**\n",
    "\n",
    "- Si *creation_dataset = True*, c'est le chemin dans lequel sera enregistré le dataset.\n",
    "- Ne pas oublier de mettre un '/' à la fin\n",
    "- Le dataset contiendra un dossier /images/ et un fichier dimensions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa79809-194a-40dd-9dcb-468efb235a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_dataset = \"/home/jbgodefroy/Documents/Data/Echantillons_A_et_D/test_algo/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731019f-292f-45c8-9b5a-7240a1de88ad",
   "metadata": {},
   "source": [
    "***\n",
    "**Chemin vers l'image contenant la grande carte**. \n",
    "\n",
    "L'extension n'importe pas (jpeg, png, tif...).  Si l'image est en couleur, elle sera convertie automatiquement en noir et blanc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61874cf6-0c05-4235-8b5e-d67ac2d5ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_grande_carte = \"/home/jbgodefroy/Documents/Data/Echantillons_A_et_D/Serie_A/ech28_reconstruction_x5_grayscale.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d54fa-81c2-444e-8193-109ee0018bfb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbdbf9-3c42-4a68-957f-2d9666ae187a",
   "metadata": {},
   "source": [
    "## 1.2 Paramètres de l'analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0544de25-235e-4e14-b02b-5c09ba02f7a1",
   "metadata": {},
   "source": [
    "***\n",
    "**Chemin et nom de fichier où enregistrer l'image de prédiction**. L'extension n'importe pas (jpeg, png, tif...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b59b1b-7148-497e-ac08-eb25aad52ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_prediction = \"/home/jbgodefroy/Documents/Data/Echantillons_A_et_D/test_algo/prediction.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24bf7a-a51e-4345-9c99-a0a82e4448d4",
   "metadata": {},
   "source": [
    "***\n",
    "**Chemin où trouver le réseau de neurones**. Le chemin peut pointer vers:\n",
    "- un fichier de réseau de neurones (extension *.keras*),\n",
    "- un dossier contenant un réseau de neurone (extension *.keras*),\n",
    "- un dossier contenant un fichier *model_info.json* et un sous-dossier *weights*. Un fichier *model.keras* sera alors automatiquement généré à partir du dossier *weights*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d3530-bb89-48fc-a2b0-8d5883a98c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_reseau_neurones = \"./trained_models/2025-02-07_Resnet_64x2x11_64x2x7_128x2x5/model_500_epochs.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89c92b-3975-411e-b012-83a665f35989",
   "metadata": {},
   "source": [
    "***\n",
    "**Seuil de prédiction et paramètres d'affichage**\n",
    "\n",
    "Après l'analyse, la prédiction est superposée à l'image. Les valeurs de prédiction vont de 0 à 255, le paramètre *seuil_prediction* permet d'afficher uniquement les zones dépassant le seuil.\n",
    "\n",
    "La meilleur valeur de *seuil_prediction* varie en fonction du réseau de neurones utilisé, je recommande de commencer à *seuil_prediction = 128* et d'ajuster après l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa82fe-6ed5-4f53-98e2-756a38026276",
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil_prediction = 0\n",
    "\n",
    "# Parfois, l'échelle des valeurs de prédiction doit être inversée, cela dépend du réseau de neurones\n",
    "inverser_prediction = False\n",
    "\n",
    "# Pour afficher la prédiction sur toutes la grande carte, et pas uniquement dans les zones classée comme contenant un liseré\n",
    "ignorer_classification = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8ac9e-59e1-44e7-8fd4-564213bfe98a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Imports et utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9b4c0-603a-453f-b946-b7eecbe439e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import datetime\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd50c4-a430-4837-9619-717a52c21456",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (400, 400)  # Specify the size of the sub-images (width x height)\n",
    "overlap = (133, 133)  # Specify the overlap between sub-images (horizontal x vertical)\n",
    "use_mask = False # True to use an annotation mask\n",
    "make_annotations = False\n",
    "side_crop = 100 # Big map side crop in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f7f71c-575c-4160-867a-a284c5d532f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=5, strides=1,\n",
    "                        padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                        use_bias=False)\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, kernel_size=5, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters, kernel_size=kernel_size),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"activation\": self.activation,\n",
    "            \"filters\": self.filters,\n",
    "            \"strides\": self.strides,\n",
    "            \"kernel_size\": self.kernel_size\n",
    "        })\n",
    "        return config   \n",
    "    \n",
    "\n",
    "class SEBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        num_channels = input_shape[-1]\n",
    "        self.squeeze = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.dense_1 = tf.keras.layers.Dense(num_channels // self.ratio, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.squeeze(inputs)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = tf.keras.layers.Reshape((1, 1, -1))(x)\n",
    "        return inputs * x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"ratio\": self.ratio})\n",
    "        return config\n",
    "\n",
    "class SE_ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", se_ratio=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "        self.se = SEBlock(ratio=se_ratio)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        Z = self.se(Z)  # Appliquer le bloc SE ici.\n",
    "        return self.activation(Z + skip_Z)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"activation\": self.activation,\n",
    "            \"filters\": self.filters,\n",
    "            \"strides\": self.strides,\n",
    "            \"se_ratio\": self.se.ratio\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class adaptateur(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, n=1):\n",
    "        super(adaptateur, self).__init__()\n",
    "\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "        self.dense = Dense(n, activation='sigmoid', bias_initializer=None, use_bias=False)\n",
    "\n",
    "    def get_weights(self, n):\n",
    "        return np.reshape(self.dense.weights[0].numpy(), n)\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65555933-25c8-4877-896d-d000ca4262c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet2(filters_sequence = [[64, 3, 5], [128, 4, 5], [256, 6, 5], [512, 3, 5]], input_shape=[400, 400, 1], first_kernel_size=7, se_blocks=False, activation='relu'):\n",
    "\n",
    "    resnet = tf.keras.Sequential([\n",
    "        DefaultConv2D(64, kernel_size=first_kernel_size, strides=2, input_shape=input_shape),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation(\"relu\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=1, padding=\"same\"),\n",
    "    ])\n",
    "\n",
    "    filters_list = [e for sublist in [[(f, kernel_size)]*n for f, n, kernel_size in filters_sequence] for e in sublist]\n",
    "    prev_filters = filters_list[0]\n",
    "\n",
    "    for filters, kernel_size in filters_list:\n",
    "        strides = 1 if filters == prev_filters else 2\n",
    "        if se_blocks:\n",
    "            resnet.add(SE_ResidualUnit(filters, strides=strides, activation=activation))\n",
    "        else:\n",
    "            resnet.add(ResidualUnit(filters, strides=strides, kernel_size=kernel_size))\n",
    "        prev_filters = filters\n",
    "\n",
    "    return resnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a88ebb-6fd7-425b-851d-64da937e2924",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Découpage de la carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30bba20-d2ad-470b-9c1e-5e15b1ea8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (400, 400)  # Specify the size of the sub-images (width x height)\n",
    "image_size = size[0]\n",
    "overlap = (133, 133)  # Specify the overlap between sub-images (horizontal x vertical)\n",
    "use_mask = False # True to use an annotation mask\n",
    "make_annotations = False\n",
    "side_crop = 100 # Big map side crop in pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006c953-0903-458d-bf08-6c4ceea67189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image, out_path, size, overlap):\n",
    "    if len(image.shape) == 3:\n",
    "        height, width, _ = image.shape\n",
    "    else:\n",
    "        height, width = image.shape\n",
    "    size_x, size_y = size\n",
    "    overlap_x, overlap_y = overlap\n",
    "\n",
    "    # Calculate the step size to spli t the image with overlap\n",
    "    step_x = size_x - overlap_x\n",
    "    step_y = size_y - overlap_y\n",
    "    i = 0\n",
    "    dx, dy = 0, 0\n",
    "    for y in range(0, height - size_y + 1, step_y):\n",
    "        dy+=1\n",
    "        for x in range(0, width - size_x + 1, step_x):\n",
    "            if dy == 1:\n",
    "                dx+=1\n",
    "            # Extract the sub-image\n",
    "            sub_image = image[y:y+size_y, x:x+size_x]\n",
    "\n",
    "            # Save the sub-image\n",
    "            filename = f\"{out_path}image_{i}_x{x}_y{y}.png\"\n",
    "\n",
    "            sub_image = cv2.normalize(sub_image, None, 0, 255, cv2.NORM_MINMAX, dtype=0)\n",
    "            cv2.imwrite(filename, sub_image)\n",
    "            i += 1\n",
    "            \n",
    "    return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35603a06-26ad-4845-91a5-c9da6a531018",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(chemin_grande_carte, cv2.IMREAD_GRAYSCALE)[side_crop:-side_crop, side_crop:-side_crop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aded0a-9bfd-45a6-bf75-f1b8a6592957",
   "metadata": {},
   "outputs": [],
   "source": [
    "if creation_dataset:\n",
    "    images_path = chemin_dataset + 'images/'\n",
    "    \n",
    "    if os.path.isdir(images_path):\n",
    "        for e in os.listdir(images_path):\n",
    "            os.unlink(images_path + e)\n",
    "    else:\n",
    "        os.mkdir(images_path)\n",
    "\n",
    "    columns, rows = split_image(image, images_path, size, overlap)\n",
    "    pd.DataFrame({'rows':rows, 'columns':columns}, index=[0]).to_csv(chemin_dataset + 'dimensions.csv')\n",
    "else:\n",
    "    rows, columns = pd.read_csv(chemin_dataset + 'dimensions.csv', index_col=0).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731c99b-2191-4f6b-a0e0-bb7b5781167f",
   "metadata": {},
   "source": [
    "# 4. Chargement du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e923827-dfe6-4513-b79c-964e87971974",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects={'ResidualUnit':ResidualUnit,\n",
    "                'SE_ResidualUnit':SE_ResidualUnit,\n",
    "                'adaptateur':adaptateur}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadad58a-25d6-4573-8454-5a1c9b08711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_reseau_neurones = pathlib.Path(chemin_reseau_neurones)\n",
    "\n",
    "if chemin_reseau_neurones.suffix == '.keras':\n",
    "    model = tf.keras.models.load_model(chemin_reseau_neurones, custom_objects=custom_objects)\n",
    "    if len(model.layers) == 3:\n",
    "        segnet, decoder = model.layers[1:]\n",
    "    else:\n",
    "        segnet, decoder = model.layers\n",
    "    \n",
    "elif chemin_reseau_neurones.is_dir():\n",
    "    found = False\n",
    "    for e  in sorted(os.listdir(chemin_reseau_neurones)):\n",
    "        if e.endswith('.keras'):\n",
    "            model = tf.keras.models.load_model(chemin_reseau_neurones.joinpath(e), custom_objects=custom_objects)\n",
    "            if len(model.layers) == 3:\n",
    "                segnet, decoder = model.layers[1:]\n",
    "            else:\n",
    "                segnet, decoder = model.layers\n",
    "            found = True\n",
    "            break        \n",
    "    if not found and chemin_reseau_neurones.joinpath('weights').is_dir() and chemin_reseau_neurones.joinpath('model_info.json').is_file():\n",
    "        with open(chemin_reseau_neurones.joinpath('model_info.json'), 'r') as f:\n",
    "            JSON = json.load(f)\n",
    "        segnet = globals()[JSON['generator']](**JSON['generator_parameters'])\n",
    "        decoder = adaptateur()\n",
    "        model = Sequential([segnet, decoder])\n",
    "        del JSON\n",
    "\n",
    "        weights = []\n",
    "        for i in range(len(model.get_weights())):\n",
    "            weights.append(np.load(chemin_reseau_neurones.joinpath('weights').joinpath(f'weights_{i}.npy')))\n",
    "        model.set_weights(weights)\n",
    "        \n",
    "        tf.keras.models.save_model(model, chemin_reseau_neurones.joinpath('model.keras'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf2706-0575-43f3-bc99-80bb08cf3006",
   "metadata": {},
   "source": [
    "# 5. Prédiction sur la carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2f3a4-6df6-4767-8272-a394625b59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects={'ResidualUnit':ResidualUnit,\n",
    "                'SE_ResidualUnit':SE_ResidualUnit,\n",
    "                'adaptateur':adaptateur}\n",
    "\n",
    "\n",
    "weights = decoder.get_weights(n=decoder.layers[1].weights[0].shape[0])\n",
    "\n",
    "image_size         = model.input.shape[1:3][0]\n",
    "segnet_output_size = segnet.output.shape[1:3][0]\n",
    "delta = int(segnet_output_size - segnet_output_size*overlap[0]/image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f620868-19a1-4d55-82c1-b7042cb7811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortbynum(s : str):\n",
    "    split = s.split('_')\n",
    "    return int(split[1])\n",
    "\n",
    "images = np.empty((len(os.listdir(chemin_dataset+'images/')), image_size, image_size))\n",
    "for i, f in enumerate(sorted(os.listdir(chemin_dataset+'images/'), key=sortbynum)):\n",
    "    images[i] = cv2.imread(chemin_dataset+'images/'+f, cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a08be5-7464-4dc6-b5d7-13a675854004",
   "metadata": {},
   "source": [
    "### Calcul de l'image moyenne (pour filtrer les effets indésirables sur les bords des images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a75318-d845-490a-a83c-fcdfe22ae694",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "\n",
    "avg = np.zeros((segnet_output_size, segnet_output_size))\n",
    "global_min = None\n",
    "for j in range(rows):\n",
    "    vis_images = images[columns*(j):columns*(j+1)]\n",
    "\n",
    "    prediction_raw = segnet(vis_images)\n",
    "    prediction = np.average(prediction_raw, weights=weights,axis=3)\n",
    "    \n",
    "    if global_min:\n",
    "        global_min = np.min([global_min, np.min(prediction)])\n",
    "    else:\n",
    "        global_min = np.min(prediction)\n",
    "    \n",
    "    for i in range(columns):\n",
    "        avg += prediction[i]\n",
    "\n",
    "avg /= (rows*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13088afe-dd42-4bc2-b2db-671f9516c8ce",
   "metadata": {},
   "source": [
    "### Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e2fc0-bfda-43dc-9cc2-c70dd3bdd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch = np.empty((rows*delta, columns*delta))\n",
    "\n",
    "empty_patch = np.ones((segnet_output_size, segnet_output_size))*global_min\n",
    "\n",
    "for j in range(rows):\n",
    "    vis_images = images[columns*(j):columns*(j+1)]\n",
    "\n",
    "    prediction_raw = segnet(vis_images)\n",
    "    prediction = np.average(prediction_raw, weights=weights,axis=3)\n",
    "    \n",
    "    predicted_labels = decoder(prediction_raw)\n",
    "    for i in range(columns):\n",
    "        \n",
    "        \n",
    "        if (not ignorer_classification) and predicted_labels[i] < thresh:\n",
    "            patch = empty_patch\n",
    "        else:\n",
    "            patch = (prediction[i]-avg) * (-1 if inverser_prediction else 1)\n",
    "            \n",
    "        stitch[j*delta:(j+1)*delta:, i*delta:(i+1)*delta] = patch[segnet_output_size-delta:, segnet_output_size-delta:]\n",
    "        #stitch[j*delta:(j+1)*delta:, i*delta:(i+1)*delta] = cv2.normalize(stitch[j*delta:(j+1)*delta:, i*delta:(i+1)*delta], None, 0, 255, cv2.NORM_MINMAX, dtype=0)\n",
    "\n",
    "stitch = np.maximum(stitch, np.min(stitch[stitch != global_min]))\n",
    "stitch = cv2.normalize(stitch, None, 0, 255, cv2.NORM_MINMAX, dtype=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c376609-6841-4a83-9be1-f4746e0827a8",
   "metadata": {},
   "source": [
    "### Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee46eca-6ed5-4f0e-9a0f-150990bf4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, segnet, decoder\n",
    "del prediction_raw, prediction, predicted_labels\n",
    "del images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6f6a6-4a99-4d02-b4ed-f60f9957a0c3",
   "metadata": {},
   "source": [
    "# 6. Superposition et sauvegarde\n",
    "\n",
    "Après avoir modifé le paramètre *seuil_prediction*, il n'est pas nécéssaire de refaire toute l'analyse pour générer la superposition avec des seuils différents:\n",
    "\n",
    "1. Modifier la valeur de *seuil_prediction* et éxécuter la case contenant *seuil_prediction* avec  **Maj**+**Entrée**\n",
    "2. Cliquer sur ce texte\n",
    "3. Appuyer sur **Maj**+**Entrée** (la case de code en dessous devrait maintenant s'afficher et être séléctionnée automatiquement)\n",
    "4. Ré-appuyer sur **Maj**+**Entrée** pour éxécuter la case ci dessous (la date est l'heure devraient s'afficher sous la case, pour confirmer que la nouvelle sauvegarde a été effectuée)\n",
    "\n",
    "Pour masquer à nouveau le code, vous pouvez cliquer sur le petit triangle à côté du **5**, en haut à gauche de cette case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead958d-5fab-4086-8d9a-f0d18ee383df",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sz = (133 + 266 * columns, 133 + 266 * rows)\n",
    "resized_stitch = cv2.resize(stitch, new_sz)\n",
    "resized_stitch = np.clip(resized_stitch, a_min=seuil_prediction, a_max=256)\n",
    "resized_stitch = cv2.normalize(resized_stitch, 0, 256,  norm_type=cv2.NORM_MINMAX, dtype=8)\n",
    "cropped_image = image[:new_sz[1], :new_sz[0]]\n",
    "\n",
    "# Ne pas tout faire d'un coup sinon le noyau crash\n",
    "blend = np.empty((new_sz[1], new_sz[0], 3), dtype=int)\n",
    "# CV2 enregistre en BGR\n",
    "blend[..., 0] = cropped_image\n",
    "blend[..., 1] = cropped_image\n",
    "blend[..., 2] = cv2.addWeighted(cropped_image, 1, resized_stitch, 1.5, 1)\n",
    "\n",
    "cv2.imwrite(chemin_prediction, blend)\n",
    "print(\"Image sauvegardée: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498c5d1-8ec3-4516-8e29-1a532dc2a499",
   "metadata": {},
   "source": [
    "# Fin !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd62e43-b02b-4b54-a772-9f4ad56057f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fin de l'éxécution: \", datetime.datetime.now())\n",
    "plt.imshow(blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460b75d-5eb1-4cd3-9196-db7488ddacb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
